{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers-GooglePlay-SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5HKh6U_3SdY"
      },
      "source": [
        "**CREATING CUSTOM DATASET - GOOGLE PLAY REVIEWS OF FPS GAMES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8rY2Ro5n-5L"
      },
      "source": [
        "!pip install -qq google-play-scraper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cax5Km23RoL"
      },
      "source": [
        "# importing necessary libraries\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from pygments import highlight\n",
        "from pygments.lexers import JsonLexer\n",
        "from pygments.formatters import TerminalFormatter\n",
        "\n",
        "from google_play_scraper import Sort, reviews, app"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7taIYjIoNOM",
        "outputId": "c43f3be5-eeae-46a7-dea1-01d8f88a053f"
      },
      "source": [
        "app_packages = [\n",
        "               \"com.activision.callofduty.shooter\", # cod\n",
        "               \"com.gameloft.android.ANMP.GloftM5HM\", # modern combat\n",
        "               \"com.dts.freefireth\", # free fire\n",
        "               \"com.gameloft.android.ANMP.GloftNOHM\", # nova legacy\n",
        "               \"com.appsomniacs.da2\" # mini militia\n",
        "]\n",
        "\n",
        "app_infos = []\n",
        "\n",
        "for app_package in tqdm(app_packages):\n",
        "    info = app(app_package, lang='en', country='in')\n",
        "    del info['comments']\n",
        "    app_infos.append(info) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00,  5.80it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpsPJzRooNJ2",
        "outputId": "f0faf222-5285-49a1-8aab-64b43948f4f1"
      },
      "source": [
        "# printing a pretty json of the received data\n",
        "def print_json(json_obj):\n",
        "    json_str = json.dumps(\n",
        "        json_obj,\n",
        "        indent=2,\n",
        "        sort_keys=True,\n",
        "        default=str\n",
        "    )\n",
        "    print(highlight(json_str, JsonLexer(), TerminalFormatter()))\n",
        "\n",
        "print_json(app_infos[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \u001b[94m\"adSupported\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m,\n",
            "  \u001b[94m\"androidVersion\"\u001b[39;49;00m: \u001b[33m\"4.3\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"androidVersionText\"\u001b[39;49;00m: \u001b[33m\"4.3 and up\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"appId\"\u001b[39;49;00m: \u001b[33m\"com.activision.callofduty.shooter\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"containsAds\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m,\n",
            "  \u001b[94m\"contentRating\"\u001b[39;49;00m: \u001b[33m\"Rated for 16+\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"contentRatingDescription\"\u001b[39;49;00m: \u001b[33m\"Strong Violence\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"currency\"\u001b[39;49;00m: \u001b[33m\"USD\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Official CALL OF DUTY\\u00ae designed exclusively for mobile phones. Play iconic multiplayer maps and modes anytime, anywhere. 100 player Battle Royale battleground? Fast 5v5 team deathmatch? Sniper vs sniper battle? Activision\\u2019s free-to-play CALL OF DUTY\\u00ae: MOBILE has it all.\\r\\n\\r\\nFREE TO PLAY ON MOBILE\\r\\n\\r\\nConsole quality HD gaming on your phone with customizable controls, voice and text chat, and thrilling 3D graphics and sound. Experience the thrill of the world\\u2019s most beloved shooter game, now on your phone for easy on-the-go fun.\\r\\n\\r\\nBELOVED GAME MODES AND MAPS\\r\\n\\r\\nPlay iconic multiplayer maps from Call of Duty\\u00ae: Black Ops and Call of Duty\\u00ae: Modern Warfare\\u00ae, available for the first time for free. Or squad up with friends in a brand new 100-person battle royale survival map. Join the fun with millions of players from all around world!\\r\\n\\r\\nCUSTOMIZE YOUR UNIQUE LOADOUT\\r\\n\\r\\nAs you play CALL OF DUTY\\u00ae: MOBILE you will unlock and earn dozens of famous characters, weapons, outfits, scorestreaks and pieces of gear that can be used to customize your loadouts. Bring these loadouts into battle in Battle Royale and thrilling PvP multiplayers modes like Team Deathmatch, Frontline, Free For All, Search and Destroy, Domination, Hardpoint and many more.\\r\\n\\r\\nCOMPETITIVE AND SOCIAL PLAY\\r\\n\\r\\nUse skill and strategy to battle to the top in competitive Ranked Mode or to win the most Clan prizes as you play with friends. Compete and fight against millions of friends and foes in this thrilling free to play multiplayer shooter.\\r\\n\\r\\nCHOICE AND COMPLEXITY\\r\\n\\r\\nWhether in gameplay, events, controls, or loadouts, CALL OF DUTY\\u00ae: MOBILE offers complexity and depth in an ever-changing experience.\\r\\n\\r\\nHave what it takes to compete with the best? Download CALL OF DUTY\\u00ae: MOBILE now!\\r\\n_________________________________________________________\\r\\nNOTE: We welcome any feedback during your experience to improve the game. To give feedback, in-game go to > Settings > Feedback > Contact Us.\\r\\n\\r\\nSubscribe for Updates! ---> profile.callofduty.com/cod/registerMobileGame\\t\\r\\n_________________________________________________________\\r\\nNote: An internet connection is required to play this game.\\r\\n\\r\\nPlease note this app contains social features that allow you to connect and play with friends and push notifications to inform you when exciting events or new content are taking place in the game. You can choose whether or not to utilize these features.\\r\\n\\r\\n\\u00a9 2019 Activision Publishing, Inc. ACTIVISION and CALL OF DUTY are trademarks of Activision Publishing, Inc. All other trademarks and trade names are the properties of their respective owners. By downloading, installing or using this App, you agree to Activision's privacy policy and terms of us, as may be updated by Activision from time to time. Please visit http://www.activision.com/privacy/en/privacy.html to view Activision's privacy policy and https://www.activision.com/legal/terms-of-use to view Activision's terms of use.\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"descriptionHTML\"\u001b[39;49;00m: \u001b[33m\"Official CALL OF DUTY\\u00ae designed exclusively for mobile phones. Play iconic multiplayer maps and modes anytime, anywhere. 100 player Battle Royale battleground? Fast 5v5 team deathmatch? Sniper vs sniper battle? Activision\\u2019s free-to-play CALL OF DUTY\\u00ae: MOBILE has it all.<br><br>FREE TO PLAY ON MOBILE<br><br>Console quality HD gaming on your phone with customizable controls, voice and text chat, and thrilling 3D graphics and sound. Experience the thrill of the world\\u2019s most beloved shooter game, now on your phone for easy on-the-go fun.<br><br>BELOVED GAME MODES AND MAPS<br><br>Play iconic multiplayer maps from Call of Duty\\u00ae: Black Ops and Call of Duty\\u00ae: Modern Warfare\\u00ae, available for the first time for free. Or squad up with friends in a brand new 100-person battle royale survival map. Join the fun with millions of players from all around world!<br><br>CUSTOMIZE YOUR UNIQUE LOADOUT<br><br>As you play CALL OF DUTY\\u00ae: MOBILE you will unlock and earn dozens of famous characters, weapons, outfits, scorestreaks and pieces of gear that can be used to customize your loadouts. Bring these loadouts into battle in Battle Royale and thrilling PvP multiplayers modes like Team Deathmatch, Frontline, Free For All, Search and Destroy, Domination, Hardpoint and many more.<br><br>COMPETITIVE AND SOCIAL PLAY<br><br>Use skill and strategy to battle to the top in competitive Ranked Mode or to win the most Clan prizes as you play with friends. Compete and fight against millions of friends and foes in this thrilling free to play multiplayer shooter.<br><br>CHOICE AND COMPLEXITY<br><br>Whether in gameplay, events, controls, or loadouts, CALL OF DUTY\\u00ae: MOBILE offers complexity and depth in an ever-changing experience.<br><br>Have what it takes to compete with the best? Download CALL OF DUTY\\u00ae: MOBILE now!<br>_________________________________________________________<br>NOTE: We welcome any feedback during your experience to improve the game. To give feedback, in-game go to &gt; Settings &gt; Feedback &gt; Contact Us.<br><br>Subscribe for Updates! ---&gt; profile.callofduty.com/cod/registerMobileGame\\t<br>_________________________________________________________<br>Note: An internet connection is required to play this game.<br><br>Please note this app contains social features that allow you to connect and play with friends and push notifications to inform you when exciting events or new content are taking place in the game. You can choose whether or not to utilize these features.<br><br>\\u00a9 2019 Activision Publishing, Inc. ACTIVISION and CALL OF DUTY are trademarks of Activision Publishing, Inc. All other trademarks and trade names are the properties of their respective owners. By downloading, installing or using this App, you agree to Activision&#39;s privacy policy and terms of us, as may be updated by Activision from time to time. Please visit http://www.activision.com/privacy/en/privacy.html to view Activision&#39;s privacy policy and https://www.activision.com/legal/terms-of-use to view Activision&#39;s terms of use.\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developer\"\u001b[39;49;00m: \u001b[33m\"Activision Publishing, Inc.\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developerAddress\"\u001b[39;49;00m: \u001b[33m\"3100 Ocean Park Blvd.\\nSanta Monica, California, 90405\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developerEmail\"\u001b[39;49;00m: \u001b[33m\"CoDMobile@activision.com\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developerId\"\u001b[39;49;00m: \u001b[33m\"Activision+Publishing,+Inc.\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developerInternalID\"\u001b[39;49;00m: \u001b[33m\"5463239933051156834\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"developerWebsite\"\u001b[39;49;00m: \u001b[33m\"http://www.activision.com\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"editorsChoice\"\u001b[39;49;00m: \u001b[34mfalse\u001b[39;49;00m,\n",
            "  \u001b[94m\"free\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m,\n",
            "  \u001b[94m\"genre\"\u001b[39;49;00m: \u001b[33m\"Action\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"genreId\"\u001b[39;49;00m: \u001b[33m\"GAME_ACTION\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"headerImage\"\u001b[39;49;00m: \u001b[33m\"https://play-lh.googleusercontent.com/Mj_JSobbkf2qcY3HEawY-UwIiHBTAyt0R3NpnY_-Zp7rLNTMWdlrr461LM1Ofv1m03TP\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"histogram\"\u001b[39;49;00m: [\n",
            "    \u001b[34m980475\u001b[39;49;00m,\n",
            "    \u001b[34m256506\u001b[39;49;00m,\n",
            "    \u001b[34m485912\u001b[39;49;00m,\n",
            "    \u001b[34m1215376\u001b[39;49;00m,\n",
            "    \u001b[34m10293148\u001b[39;49;00m\n",
            "  ],\n",
            "  \u001b[94m\"icon\"\u001b[39;49;00m: \u001b[33m\"https://play-lh.googleusercontent.com/9Y-xblw8XUBtnjdS5OM2v93_XQ2i0dQtWXzbjnR0XMl3hpTfeZAZL-hllTH5loBjdoo\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"inAppProductPrice\"\u001b[39;49;00m: \u001b[33m\"\\u20b910.00 - \\u20b97,900.00 per item\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"installs\"\u001b[39;49;00m: \u001b[33m\"100,000,000+\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"minInstalls\"\u001b[39;49;00m: \u001b[34m100000000\u001b[39;49;00m,\n",
            "  \u001b[94m\"offersIAP\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m,\n",
            "  \u001b[94m\"originalPrice\"\u001b[39;49;00m: \u001b[34mnull\u001b[39;49;00m,\n",
            "  \u001b[94m\"price\"\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\n",
            "  \u001b[94m\"privacyPolicy\"\u001b[39;49;00m: \u001b[33m\"https://www.activision.com/legal/privacy-policy\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"ratings\"\u001b[39;49;00m: \u001b[34m13231417\u001b[39;49;00m,\n",
            "  \u001b[94m\"recentChanges\"\u001b[39;49;00m: \u001b[33m\"Only the strongest samurai will survive Call of Duty: Mobile Season 3: Tokyo Escape! Slice and dice your way through Swords & Stones, a chaotic new melee Free-for-All! Leave no trace on the brand new Search & Destroy map Coastal and dominate the sand-swept map Oasis. Looking for more than a katana? Snag the Epic RUS-79U - Karuta and 50 Tiers of rewards in the Season 3 Battle Pass. Do you have what it takes to master the ultimate Escape? Play now!\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"recentChangesHTML\"\u001b[39;49;00m: \u001b[33m\"Only the strongest samurai will survive Call of Duty: Mobile Season 3: Tokyo Escape! Slice and dice your way through Swords &amp; Stones, a chaotic new melee Free-for-All! Leave no trace on the brand new Search &amp; Destroy map Coastal and dominate the sand-swept map Oasis. Looking for more than a katana? Snag the Epic RUS-79U - Karuta and 50 Tiers of rewards in the Season 3 Battle Pass. Do you have what it takes to master the ultimate Escape? Play now!\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"released\"\u001b[39;49;00m: \u001b[33m\"Sep 30, 2019\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"reviews\"\u001b[39;49;00m: \u001b[34m6013455\u001b[39;49;00m,\n",
            "  \u001b[94m\"sale\"\u001b[39;49;00m: \u001b[34mfalse\u001b[39;49;00m,\n",
            "  \u001b[94m\"saleText\"\u001b[39;49;00m: \u001b[34mnull\u001b[39;49;00m,\n",
            "  \u001b[94m\"saleTime\"\u001b[39;49;00m: \u001b[34mnull\u001b[39;49;00m,\n",
            "  \u001b[94m\"score\"\u001b[39;49;00m: \u001b[34m4.4801297\u001b[39;49;00m,\n",
            "  \u001b[94m\"screenshots\"\u001b[39;49;00m: [\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/k0JLJS4zQIXOpe3zfrARxXiCr_qH7nsa5LdCr5sB_cwsRNW0sGCwh8UR_Z9bgdQ4Vls\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/GFA4YkjaFrDiWWdbOync92r163x-QMB3FGaCONHmUSLRjd8cJVHCVrnD380y5US2ax8A\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/CoUQbJ8v2Kkiyl2N4sOAsF_yVixsiIoGpbHK9Ij8Mp8KMcWayF67cQ8YAmKxfT4tlgY1\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/IgGiyi9jEGzu3rU9GXsCN5UKrXV-ES2WTLgxCIDO42EWQ52dRc0Cv8C5k2jv5Ywy3xw\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/CoUmsq-1aNEhsAHmwrXy9vbG9tK_4vIPQ-TJlIFSZBXWXpJBvOXNBQVdyytwT0xVSQ\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/NttFc14krmjJL4KX3Ceq07hLl0VS_btJOIVhE7zlvyDpFMCVpAhULVYGKCLbhbns4B8\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/5XnODB1cQ7LR2WyFXqskSZ87T3IiRaBiUvl2MRw4Q7WXVaNQi4erLjvJSo2T3rECpXh6\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/t5oeiArOj1ud5N_FcP_rb4EDW1Jfb9hdduZymL3HPeqihlnEEChx0-vrBctx-NyS1Oo\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/7pPc0m23mJ2QLRkWqpFGhHsXvsYdldBErhAoIGOfv0oLlnZR_dgawlJkCDdr8xqsM1M\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/4WaEyWs11fAahwANdyfF76VcFNoZP5PecNvuB3-c9tiFmaIDId4er6CVYUZpoNZNavg\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/L8lf1g3C7LNogiG6TJJGv_jZr9-4XZ2WH_gbcFPIIM3ZVA3ctwMEsBjVH-JKDJnET9ga\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/UuWHBpa0HT3ItEmQJESlkw4bJrXgv0ruY7R1DJcOxMr0h-fToVIz-vt7Yzgw9vG9HQ\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/_5xHCAwDbwh1upA0o8H8JFwfFvRbQSdrMBUc71mn-B97ExGoXQGEeee75IYGio64x0o\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/OB2-ZJlgOcXanDk_utrhMHHwHPhgRszpePuTCBJdtaMOccaDG7hCh2iqZBHJ-Pf8U_rG\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/xe_VyCFqRjxdHGVLbWIBt7oN7t_JSGYEIYIETprxeBwsyob29JXFm-ilNJkrcmWnBpiu\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/BBCVAUzpGHX4uKaVy9taZQ0iV1APUKjqWNexkiOeWr4joVEVUaDcpu_EG4Un82PrC5Y\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/LkIKfrzlJX8-pK83ssJAwNUjtAEfph-7JkKiGHjz1CZdJb_LOVLRZEhpAcHTnhYp_iI\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/Z6o9aeXBRjx5dz0-KBss7chA2hh6QwaSvDa9WSSYVTVYuFwfhMKxocq_BgAgyXw5RAc\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/RWHJU2i-M7n36v7J-fX47JNRj2Eb5GgQb7SSt1hBZ3B85FDEWh0DJ-BmcGzQ9XHqylpY\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/84bwgoeZ5RdQ_XpDhTWVItLT_gBBzsJpaep6gkXIExKY5keQo6_1cgdZ8iFDpe_6y-A\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/mIUwSVozE1786jUNcQqKQNxrnE9wqUkYfwBx_-O8bMXZtchNfN77QLHTee_wZ-mRiw\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/VVrJe4KDnSzhj5Qf2UFZDIgVtacWB7yDYZlWjcBtfYPDqxMkZDFdGONJoWlkeL_LD04\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/QnXRb2wyvTf2K5So5w0zzUCkh3vDISEkceY7BG_ZNOO3IWIuKo0MYdFE-cqcBMz09xY\"\u001b[39;49;00m,\n",
            "    \u001b[33m\"https://play-lh.googleusercontent.com/Pl8jSwKupDJa-gTGVLWkXliKsqqA9_y_Uzn4SCDAm-REhdTj7GQqacjqLRp_dl3Qng\"\u001b[39;49;00m\n",
            "  ],\n",
            "  \u001b[94m\"size\"\u001b[39;49;00m: \u001b[33m\"91M\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"summary\"\u001b[39;49;00m: \u001b[33m\"Call of Duty: Mobile offers PVP, Battle Royale, Sniper gameplay\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"summaryHTML\"\u001b[39;49;00m: \u001b[33m\"Call of Duty: Mobile offers PVP, Battle Royale, Sniper gameplay\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"title\"\u001b[39;49;00m: \u001b[33m\"Call of Duty\\u00ae: Mobile - Tokyo Escape\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"updated\"\u001b[39;49;00m: \u001b[34m1615073414\u001b[39;49;00m,\n",
            "  \u001b[94m\"url\"\u001b[39;49;00m: \u001b[33m\"https://play.google.com/store/apps/details?id=com.activision.callofduty.shooter&hl=en&gl=in\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"version\"\u001b[39;49;00m: \u001b[33m\"1.0.20\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"video\"\u001b[39;49;00m: \u001b[33m\"https://www.youtube.com/embed/gFKhZToY-LM?ps=play&vq=large&rel=0&autohide=1&showinfo=0\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"videoImage\"\u001b[39;49;00m: \u001b[33m\"https://i.ytimg.com/vi/gFKhZToY-LM/hqdefault.jpg\"\u001b[39;49;00m\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "Wx8RZ_wmoNHx",
        "outputId": "e6cf060c-4f4e-493c-a0d0-7b5b485a7c56"
      },
      "source": [
        "# converting json file to pandas df\n",
        "df = pd.DataFrame(app_infos)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>descriptionHTML</th>\n",
              "      <th>summary</th>\n",
              "      <th>summaryHTML</th>\n",
              "      <th>installs</th>\n",
              "      <th>minInstalls</th>\n",
              "      <th>score</th>\n",
              "      <th>ratings</th>\n",
              "      <th>reviews</th>\n",
              "      <th>histogram</th>\n",
              "      <th>price</th>\n",
              "      <th>free</th>\n",
              "      <th>currency</th>\n",
              "      <th>sale</th>\n",
              "      <th>saleTime</th>\n",
              "      <th>originalPrice</th>\n",
              "      <th>saleText</th>\n",
              "      <th>offersIAP</th>\n",
              "      <th>inAppProductPrice</th>\n",
              "      <th>size</th>\n",
              "      <th>androidVersion</th>\n",
              "      <th>androidVersionText</th>\n",
              "      <th>developer</th>\n",
              "      <th>developerId</th>\n",
              "      <th>developerEmail</th>\n",
              "      <th>developerWebsite</th>\n",
              "      <th>developerAddress</th>\n",
              "      <th>privacyPolicy</th>\n",
              "      <th>developerInternalID</th>\n",
              "      <th>genre</th>\n",
              "      <th>genreId</th>\n",
              "      <th>icon</th>\n",
              "      <th>headerImage</th>\n",
              "      <th>screenshots</th>\n",
              "      <th>video</th>\n",
              "      <th>videoImage</th>\n",
              "      <th>contentRating</th>\n",
              "      <th>contentRatingDescription</th>\n",
              "      <th>adSupported</th>\n",
              "      <th>containsAds</th>\n",
              "      <th>released</th>\n",
              "      <th>updated</th>\n",
              "      <th>version</th>\n",
              "      <th>recentChanges</th>\n",
              "      <th>recentChangesHTML</th>\n",
              "      <th>editorsChoice</th>\n",
              "      <th>appId</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Call of Duty®: Mobile - Tokyo Escape</td>\n",
              "      <td>Official CALL OF DUTY® designed exclusively fo...</td>\n",
              "      <td>Official CALL OF DUTY® designed exclusively fo...</td>\n",
              "      <td>Call of Duty: Mobile offers PVP, Battle Royale...</td>\n",
              "      <td>Call of Duty: Mobile offers PVP, Battle Royale...</td>\n",
              "      <td>100,000,000+</td>\n",
              "      <td>100000000</td>\n",
              "      <td>4.480130</td>\n",
              "      <td>13231417</td>\n",
              "      <td>6013455</td>\n",
              "      <td>[980475, 256506, 485912, 1215376, 10293148]</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>USD</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>₹10.00 - ₹7,900.00 per item</td>\n",
              "      <td>91M</td>\n",
              "      <td>4.3</td>\n",
              "      <td>4.3 and up</td>\n",
              "      <td>Activision Publishing, Inc.</td>\n",
              "      <td>Activision+Publishing,+Inc.</td>\n",
              "      <td>CoDMobile@activision.com</td>\n",
              "      <td>http://www.activision.com</td>\n",
              "      <td>3100 Ocean Park Blvd.\\nSanta Monica, Californi...</td>\n",
              "      <td>https://www.activision.com/legal/privacy-policy</td>\n",
              "      <td>5463239933051156834</td>\n",
              "      <td>Action</td>\n",
              "      <td>GAME_ACTION</td>\n",
              "      <td>https://play-lh.googleusercontent.com/9Y-xblw8...</td>\n",
              "      <td>https://play-lh.googleusercontent.com/Mj_JSobb...</td>\n",
              "      <td>[https://play-lh.googleusercontent.com/k0JLJS4...</td>\n",
              "      <td>https://www.youtube.com/embed/gFKhZToY-LM?ps=p...</td>\n",
              "      <td>https://i.ytimg.com/vi/gFKhZToY-LM/hqdefault.jpg</td>\n",
              "      <td>Rated for 16+</td>\n",
              "      <td>Strong Violence</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Sep 30, 2019</td>\n",
              "      <td>1615073414</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>Only the strongest samurai will survive Call o...</td>\n",
              "      <td>Only the strongest samurai will survive Call o...</td>\n",
              "      <td>False</td>\n",
              "      <td>com.activision.callofduty.shooter</td>\n",
              "      <td>https://play.google.com/store/apps/details?id=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Modern Combat 5: eSports FPS</td>\n",
              "      <td>Start shooting with the game that has it all: ...</td>\n",
              "      <td>Start shooting with the game that has it all: ...</td>\n",
              "      <td>The best multiplayer FPS series raises the bar...</td>\n",
              "      <td>The best multiplayer FPS series raises the bar...</td>\n",
              "      <td>100,000,000+</td>\n",
              "      <td>100000000</td>\n",
              "      <td>4.236816</td>\n",
              "      <td>3494539</td>\n",
              "      <td>1445458</td>\n",
              "      <td>[377414, 102909, 215384, 417824, 2381008]</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>USD</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>₹10.00 - ₹8,900.00 per item</td>\n",
              "      <td>52M</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0 and up</td>\n",
              "      <td>Gameloft SE</td>\n",
              "      <td>4826827787946964969</td>\n",
              "      <td>android.support@gameloft.com</td>\n",
              "      <td>http://www.gameloft.com/</td>\n",
              "      <td>14 rue Auber 75009 Paris</td>\n",
              "      <td>http://www.gameloft.com/privacy-notice/</td>\n",
              "      <td>4826827787946964969</td>\n",
              "      <td>Action</td>\n",
              "      <td>GAME_ACTION</td>\n",
              "      <td>https://play-lh.googleusercontent.com/WPP5y9dz...</td>\n",
              "      <td>https://play-lh.googleusercontent.com/tPYOrMZT...</td>\n",
              "      <td>[https://play-lh.googleusercontent.com/nNjmESX...</td>\n",
              "      <td>https://www.youtube.com/embed/Bmuzjul8wS4?ps=p...</td>\n",
              "      <td>https://i.ytimg.com/vi/Bmuzjul8wS4/hqdefault.jpg</td>\n",
              "      <td>Rated for 16+</td>\n",
              "      <td>Strong Violence, Strong Language</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Jul 23, 2014</td>\n",
              "      <td>1614602255</td>\n",
              "      <td>5.8.1c</td>\n",
              "      <td>Welcome to this new update!\\r\\n\\r\\nFlamed weap...</td>\n",
              "      <td>Welcome to this new update!&lt;br&gt;&lt;br&gt;Flamed weap...</td>\n",
              "      <td>True</td>\n",
              "      <td>com.gameloft.android.ANMP.GloftM5HM</td>\n",
              "      <td>https://play.google.com/store/apps/details?id=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Garena Free Fire- World Series</td>\n",
              "      <td>Free Fire is the ultimate survival shooter gam...</td>\n",
              "      <td>Free Fire is the ultimate survival shooter gam...</td>\n",
              "      <td>10-minute Survival Shooter!</td>\n",
              "      <td>10-minute Survival Shooter!</td>\n",
              "      <td>500,000,000+</td>\n",
              "      <td>500000000</td>\n",
              "      <td>4.295106</td>\n",
              "      <td>84138354</td>\n",
              "      <td>49968623</td>\n",
              "      <td>[10669833, 1973644, 2988722, 4730932, 63775223]</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>USD</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>₹10.00 - ₹9,000.00 per item</td>\n",
              "      <td>Varies with device</td>\n",
              "      <td>4.1</td>\n",
              "      <td>4.1 and up</td>\n",
              "      <td>GARENA INTERNATIONAL I PRIVATE LIMITED</td>\n",
              "      <td>GARENA+INTERNATIONAL+I+PRIVATE+LIMITED</td>\n",
              "      <td>freefire@garena.com</td>\n",
              "      <td>https://ff.garena.com</td>\n",
              "      <td>1 FUSIONOPOLIS PLACE, 17-10, GALAXIS, Singapore</td>\n",
              "      <td>https://ff.garena.com/others/policy/en/</td>\n",
              "      <td>5262113047916054197</td>\n",
              "      <td>Action</td>\n",
              "      <td>GAME_ACTION</td>\n",
              "      <td>https://play-lh.googleusercontent.com/NE5J766Y...</td>\n",
              "      <td>https://play-lh.googleusercontent.com/KxIKOXKi...</td>\n",
              "      <td>[https://play-lh.googleusercontent.com/-T70lja...</td>\n",
              "      <td>https://www.youtube.com/embed/hMBx5jkNi1s?ps=p...</td>\n",
              "      <td>https://i.ytimg.com/vi/hMBx5jkNi1s/hqdefault.jpg</td>\n",
              "      <td>Rated for 12+</td>\n",
              "      <td>Moderate Violence</td>\n",
              "      <td>None</td>\n",
              "      <td>False</td>\n",
              "      <td>Nov 20, 2017</td>\n",
              "      <td>1617957191</td>\n",
              "      <td>1.60.1</td>\n",
              "      <td>1. Clash Squad Season 6 - Begins 04/15 17:00 G...</td>\n",
              "      <td>1. Clash Squad Season 6 - Begins 04/15 17:00 G...</td>\n",
              "      <td>True</td>\n",
              "      <td>com.dts.freefireth</td>\n",
              "      <td>https://play.google.com/store/apps/details?id=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>N.O.V.A. Legacy</td>\n",
              "      <td>THE LEGEND REBORN &amp; REMASTERED\\r\\n\\r\\nN.O.V.A....</td>\n",
              "      <td>THE LEGEND REBORN &amp;amp; REMASTERED&lt;br&gt;&lt;br&gt;N.O....</td>\n",
              "      <td>N.O.V.A. Legacy brings you the best sci-fi FPS...</td>\n",
              "      <td>N.O.V.A. Legacy brings you the best sci-fi FPS...</td>\n",
              "      <td>50,000,000+</td>\n",
              "      <td>50000000</td>\n",
              "      <td>4.063901</td>\n",
              "      <td>1506573</td>\n",
              "      <td>707840</td>\n",
              "      <td>[207618, 57077, 109724, 189147, 943007]</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>USD</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>₹10.00 - ₹8,900.00 per item</td>\n",
              "      <td>45M</td>\n",
              "      <td>4.0.3</td>\n",
              "      <td>4.0.3 and up</td>\n",
              "      <td>Gameloft SE</td>\n",
              "      <td>4826827787946964969</td>\n",
              "      <td>android.support@gameloft.com</td>\n",
              "      <td>http://www.gameloft.com/</td>\n",
              "      <td>14 rue Auber 75009 Paris</td>\n",
              "      <td>http://www.gameloft.com/privacy-notice/</td>\n",
              "      <td>4826827787946964969</td>\n",
              "      <td>Action</td>\n",
              "      <td>GAME_ACTION</td>\n",
              "      <td>https://play-lh.googleusercontent.com/4OvZs26g...</td>\n",
              "      <td>https://play-lh.googleusercontent.com/hVuOGFxp...</td>\n",
              "      <td>[https://play-lh.googleusercontent.com/DQppPRL...</td>\n",
              "      <td>https://www.youtube.com/embed/RRViSISYi2g?ps=p...</td>\n",
              "      <td>https://i.ytimg.com/vi/RRViSISYi2g/hqdefault.jpg</td>\n",
              "      <td>Rated for 7+</td>\n",
              "      <td>Implied Violence, Mild Violence</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mar 27, 2017</td>\n",
              "      <td>1602767306</td>\n",
              "      <td>5.8.3c</td>\n",
              "      <td>Minor bugs fixed</td>\n",
              "      <td>Minor bugs fixed</td>\n",
              "      <td>False</td>\n",
              "      <td>com.gameloft.android.ANMP.GloftNOHM</td>\n",
              "      <td>https://play.google.com/store/apps/details?id=...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mini Militia - Doodle Army 2</td>\n",
              "      <td>Mini Militia - Doodle Army 2 is all about inte...</td>\n",
              "      <td>Mini Militia - Doodle Army 2 is all about inte...</td>\n",
              "      <td>One of the most addicting and fun multiplayer ...</td>\n",
              "      <td>One of the most addicting and fun multiplayer ...</td>\n",
              "      <td>100,000,000+</td>\n",
              "      <td>100000000</td>\n",
              "      <td>4.122235</td>\n",
              "      <td>3094629</td>\n",
              "      <td>1313684</td>\n",
              "      <td>[445895, 103798, 172891, 275600, 2096445]</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>USD</td>\n",
              "      <td>False</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>True</td>\n",
              "      <td>₹34.00 - ₹2,965.00 per item</td>\n",
              "      <td>43M</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.4 and up</td>\n",
              "      <td>Miniclip.com</td>\n",
              "      <td>5933611429942957630</td>\n",
              "      <td>support@miniclip.com</td>\n",
              "      <td>https://support.miniclip.com/</td>\n",
              "      <td>Miniclip SA\\nCase Postale 2671\\n2001 Neuchâtel...</td>\n",
              "      <td>http://www.miniclip.com/android/privacy-policy/</td>\n",
              "      <td>5933611429942957630</td>\n",
              "      <td>Action</td>\n",
              "      <td>GAME_ACTION</td>\n",
              "      <td>https://play-lh.googleusercontent.com/SXaV56Ie...</td>\n",
              "      <td>https://play-lh.googleusercontent.com/eZHurmqY...</td>\n",
              "      <td>[https://play-lh.googleusercontent.com/_tdZWhz...</td>\n",
              "      <td>https://www.youtube.com/embed/Xtrqu1HK4Ww?ps=p...</td>\n",
              "      <td>https://i.ytimg.com/vi/Xtrqu1HK4Ww/hqdefault.jpg</td>\n",
              "      <td>Rated for 12+</td>\n",
              "      <td>Moderate Violence</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>Mar 13, 2015</td>\n",
              "      <td>1606491104</td>\n",
              "      <td>5.3.4</td>\n",
              "      <td>- In-game Friends is here! Now you can add pla...</td>\n",
              "      <td>- In-game Friends is here! Now you can add pla...</td>\n",
              "      <td>False</td>\n",
              "      <td>com.appsomniacs.da2</td>\n",
              "      <td>https://play.google.com/store/apps/details?id=...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  title  ...                                                url\n",
              "0  Call of Duty®: Mobile - Tokyo Escape  ...  https://play.google.com/store/apps/details?id=...\n",
              "1          Modern Combat 5: eSports FPS  ...  https://play.google.com/store/apps/details?id=...\n",
              "2        Garena Free Fire- World Series  ...  https://play.google.com/store/apps/details?id=...\n",
              "3                       N.O.V.A. Legacy  ...  https://play.google.com/store/apps/details?id=...\n",
              "4          Mini Militia - Doodle Army 2  ...  https://play.google.com/store/apps/details?id=...\n",
              "\n",
              "[5 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPzNT6qBoNGF"
      },
      "source": [
        "# saving pandas df\n",
        "\n",
        "df.to_csv('/content/apps.csv', index=None, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy3GFIozoNEn",
        "outputId": "ae567e99-3b68-4fec-9c6e-685b39c396f2"
      },
      "source": [
        "# getting apps reviews\n",
        "\n",
        "app_reviews = []\n",
        "\n",
        "for app_package in tqdm(app_packages):\n",
        "    for score in range(1, 6):\n",
        "        for sort_order in [Sort.MOST_RELEVANT, Sort.NEWEST]:\n",
        "            rvs = reviews(\n",
        "                app_package, \n",
        "                lang='en', \n",
        "                country='in', \n",
        "                sort=sort_order, \n",
        "                count = 200 if score == 3 else 100,\n",
        "                filter_score_with=score\n",
        "            )\n",
        "            \n",
        "            app_reviews.extend(rvs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:16<00:00,  3.27s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsgRI5nYoNCv",
        "outputId": "9b0203e5-6b78-4892-c22c-57700aba92bf"
      },
      "source": [
        "len(app_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FurmryREoNA8",
        "outputId": "3180c282-21d4-480e-9643-cd6dabcc1d03"
      },
      "source": [
        "print_json(app_reviews[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \u001b[94m\"at\"\u001b[39;49;00m: \u001b[33m\"2021-04-25 07:36:39\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"content\"\u001b[39;49;00m: \u001b[33m\"Game keeps on crashing. I get a black screen, with the sound and everything still running. If I minimise and reopen it, the game restarts. In a 3hrs gameplay, this has happened more than 5 times. I'm having Poco X3 Pro brand new mobile and my internet connection is good. This is seriously annoying when the game crashes. Please fix this as soon as possible.\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"repliedAt\"\u001b[39;49;00m: \u001b[34mnull\u001b[39;49;00m,\n",
            "  \u001b[94m\"replyContent\"\u001b[39;49;00m: \u001b[34mnull\u001b[39;49;00m,\n",
            "  \u001b[94m\"reviewCreatedVersion\"\u001b[39;49;00m: \u001b[33m\"1.0.20\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"reviewId\"\u001b[39;49;00m: \u001b[33m\"gp:AOqpTOFDlMgcL0EIskBGj5Twl61m8xDXLPU26EHNEkQ0U-jowMAC26clgQmBz8VDomhw0dW6_Js5SJPSJSFXdg\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"score\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
            "  \u001b[94m\"thumbsUpCount\"\u001b[39;49;00m: \u001b[34m8\u001b[39;49;00m,\n",
            "  \u001b[94m\"userImage\"\u001b[39;49;00m: \u001b[33m\"https://play-lh.googleusercontent.com/--W6G1vdxNEE/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucmYdc5El_x9SXnn4K92pBmpkGrHJA/photo.jpg\"\u001b[39;49;00m,\n",
            "  \u001b[94m\"userName\"\u001b[39;49;00m: \u001b[33m\"Andruraj A\"\u001b[39;49;00m\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "id": "yJ88Xj6woM-v",
        "outputId": "1c136a89-3693-475f-9f49-de7c817913b8"
      },
      "source": [
        "# converting json list of reviews to pandas df\n",
        "\n",
        "reviews_df = pd.DataFrame(app_reviews)\n",
        "\n",
        "reviews_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewId</th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gp:AOqpTOFDlMgcL0EIskBGj5Twl61m8xDXLPU26EHNEkQ...</td>\n",
              "      <td>Andruraj A</td>\n",
              "      <td>https://play-lh.googleusercontent.com/--W6G1vd...</td>\n",
              "      <td>Game keeps on crashing. I get a black screen, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>2021-04-25 07:36:39</td>\n",
              "      <td>None</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gp:AOqpTOF7wi2nwkP84he3gsaeKsulE30CeLXKLlf4N8Q...</td>\n",
              "      <td>Supreeth Tikare</td>\n",
              "      <td>https://play-lh.googleusercontent.com/a-/AOh14...</td>\n",
              "      <td>After this new update game gets closed saying ...</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>2021-04-25 06:36:13</td>\n",
              "      <td>None</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gp:AOqpTOGkQ9mTIwPNQ9UPJ5mb3YKIkkXtjYAbmk7otGB...</td>\n",
              "      <td>Abhijit Kasbekar</td>\n",
              "      <td>https://play-lh.googleusercontent.com/a-/AOh14...</td>\n",
              "      <td>Despite having a phone with 8gb Ram and latest...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>2021-04-25 06:59:09</td>\n",
              "      <td>None</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gp:AOqpTOF6M1kcZSrJ3KXXuTJpFq8p8hflqQae23u-Kwn...</td>\n",
              "      <td>Prantik Mondal</td>\n",
              "      <td>https://play-lh.googleusercontent.com/a-/AOh14...</td>\n",
              "      <td>Since last update, most of the time game not o...</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>2021-04-24 15:03:21</td>\n",
              "      <td>None</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>gp:AOqpTOF1ipsEwcLhDTNsIn12lJOdcyF6KAXDe6zINwi...</td>\n",
              "      <td>NL Paradox</td>\n",
              "      <td>https://play-lh.googleusercontent.com/a-/AOh14...</td>\n",
              "      <td>The game wasn't bad when I first started it ab...</td>\n",
              "      <td>1</td>\n",
              "      <td>1461</td>\n",
              "      <td>1.0.20</td>\n",
              "      <td>2021-03-27 12:56:36</td>\n",
              "      <td>None</td>\n",
              "      <td>NaT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            reviewId  ... repliedAt\n",
              "0  gp:AOqpTOFDlMgcL0EIskBGj5Twl61m8xDXLPU26EHNEkQ...  ...       NaT\n",
              "1  gp:AOqpTOF7wi2nwkP84he3gsaeKsulE30CeLXKLlf4N8Q...  ...       NaT\n",
              "2  gp:AOqpTOGkQ9mTIwPNQ9UPJ5mb3YKIkkXtjYAbmk7otGB...  ...       NaT\n",
              "3  gp:AOqpTOF6M1kcZSrJ3KXXuTJpFq8p8hflqQae23u-Kwn...  ...       NaT\n",
              "4  gp:AOqpTOF1ipsEwcLhDTNsIn12lJOdcyF6KAXDe6zINwi...  ...       NaT\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whDCb1QRJgVP",
        "outputId": "6d56b2d2-9a7d-426c-8903-94a1dc5f6675"
      },
      "source": [
        "# shape of final df\n",
        "\n",
        "reviews_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V79TliUoM8z"
      },
      "source": [
        "# saving reviews df to csv\n",
        "\n",
        "reviews_df.to_csv('reviews_df', index=None, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADKunetUKXFH"
      },
      "source": [
        "**TEXT FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJHXmhlVyoDO"
      },
      "source": [
        "reviews_df = pd.read_csv(\"/content/reviews_df\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o81YudzpoM6X",
        "outputId": "332ec402-3c87-4e7d-f842-b1f9b512914a"
      },
      "source": [
        "# our final df is \"reviews_df\", important cols are = 'content' and 'score'\n",
        "\n",
        "reviews_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6000 entries, 0 to 5999\n",
            "Data columns (total 10 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   reviewId              6000 non-null   object\n",
            " 1   userName              6000 non-null   object\n",
            " 2   userImage             6000 non-null   object\n",
            " 3   content               6000 non-null   object\n",
            " 4   score                 6000 non-null   int64 \n",
            " 5   thumbsUpCount         6000 non-null   int64 \n",
            " 6   reviewCreatedVersion  4484 non-null   object\n",
            " 7   at                    6000 non-null   object\n",
            " 8   replyContent          39 non-null     object\n",
            " 9   repliedAt             39 non-null     object\n",
            "dtypes: int64(2), object(8)\n",
            "memory usage: 468.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUuh3t_cJc9q",
        "outputId": "26c0a76c-b62f-4662-ff15-84a333f0f41f"
      },
      "source": [
        "# distribution of target classes\n",
        "\n",
        "reviews_df['score'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    2000\n",
              "2    1000\n",
              "5    1000\n",
              "1    1000\n",
              "4    1000\n",
              "Name: score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "I9CoEAoaKigT",
        "outputId": "3fd403bd-1bf4-4d5c-ed7d-a03469a9e7a5"
      },
      "source": [
        "# another method to analize distribution of target classes using seaborn\n",
        "\n",
        "sns.countplot(reviews_df.score)\n",
        "plt.xlabel('review score')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'review score')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXyyh_XXKicV"
      },
      "source": [
        "# now we will group together the scores (1 and 2) as negative, 3 alone as neutral, (4 and 5) as positive.\n",
        "\n",
        "def convert_sentiments(rating_score):\n",
        "    rating_score = int(rating_score)\n",
        "    if rating_score <= 2:\n",
        "        return 0\n",
        "    elif rating_score == 3:\n",
        "        return 1\n",
        "    else: return 2\n",
        "\n",
        "reviews_df['sentiment'] = reviews_df['score'].apply(lambda x: convert_sentiments(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpNp_mp3OnW8"
      },
      "source": [
        "output_classes = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "ncwPqlJ5KiZ_",
        "outputId": "8659d5d9-9535-45f7-b4c6-b9d00f94975b"
      },
      "source": [
        "ax = sns.countplot(reviews_df['sentiment'])\n",
        "plt.xlabel('review score')\n",
        "ax.set_xticklabels(output_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0, 'negative'), Text(0, 0, 'neutral'), Text(0, 0, 'positive')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5qWTI3GPys1"
      },
      "source": [
        "**DATA PREPROCESSING FOR BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3XQrZqVKiXk"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIQ9SFCUKiVy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BxcyHvf4Y4S"
      },
      "source": [
        "Transformer Bert Base Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTrtt3uqP1oW"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-cased') # cased = if capital, remains capital"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn0H1fiFP1qP",
        "outputId": "296774d1-6bed-4768-ccce-108703137232"
      },
      "source": [
        "# let's get a trailer of what this tokenizer can do ->\n",
        "\n",
        "sample_text = 'I just want success in my life and lots of money so I can travel the world'\n",
        "print(f'sample text : {sample_text}')\n",
        "\n",
        "tokens = tokenizer.tokenize(sample_text)\n",
        "print(f'tokens ({len(tokens)}) : {tokens}')\n",
        "\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f'token ids ({len(token_ids)}) : {token_ids}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample text : I just want success in my life and lots of money so I can travel the world\n",
            "tokens (17) : ['I', 'just', 'want', 'success', 'in', 'my', 'life', 'and', 'lots', 'of', 'money', 'so', 'I', 'can', 'travel', 'the', 'world']\n",
            "token ids (17) : [146, 1198, 1328, 2244, 1107, 1139, 1297, 1105, 7424, 1104, 1948, 1177, 146, 1169, 3201, 1103, 1362]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_me79M0P1t3",
        "outputId": "2ebe530d-1934-4557-cbe9-a818fa271ee3"
      },
      "source": [
        "# special tokens needed for BERT sequence classification ->\n",
        "\n",
        "print(f'Separation Token : {tokenizer.sep_token}')\n",
        "print(f'Separation Token Id : {tokenizer.sep_token_id}')\n",
        "print()\n",
        "print(f'Classification Token : {tokenizer.cls_token}')\n",
        "print(f'Classification Token Id : {tokenizer.cls_token_id}')\n",
        "print()\n",
        "print(f'Padding Token : {tokenizer.pad_token}')\n",
        "print(f'Padding Token Id : {tokenizer.pad_token_id}')\n",
        "print()\n",
        "print(f'Unkown Token : {tokenizer.unk_token}')\n",
        "print(f'Unkown Token Id : {tokenizer.unk_token_id}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Separation Token : [SEP]\n",
            "Separation Token Id : 102\n",
            "\n",
            "Classification Token : [CLS]\n",
            "Classification Token Id : 101\n",
            "\n",
            "Padding Token : [PAD]\n",
            "Padding Token Id : 0\n",
            "\n",
            "Unkown Token : [UNK]\n",
            "Unkown Token Id : 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLc_NW3sSZ6f",
        "outputId": "6c7756de-c1ee-4520-f9ac-aeb289598619"
      },
      "source": [
        "# encoding of text to BERT input\n",
        "\n",
        "encoding = tokenizer.encode_plus(\n",
        "    sample_text,\n",
        "    max_length=20,\n",
        "    add_special_tokens=True,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=True,\n",
        "    return_token_type_ids=False,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0daRskJMSZ4c",
        "outputId": "ee2bcd0a-10e0-4c2b-e39f-03e61ab2481f"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "print(encoding['input_ids'])\n",
        "print()\n",
        "print(len(encoding['attention_mask'][0]))\n",
        "print(encoding['attention_mask'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "tensor([[ 101,  146, 1198, 1328, 2244, 1107, 1139, 1297, 1105, 7424, 1104, 1948,\n",
            "         1177,  146, 1169, 3201, 1103, 1362,  102,    0]])\n",
            "\n",
            "20\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "sisfK8YfSZ2S",
        "outputId": "3fccadd3-9044-4a06-edb4-6cfe352eb6b2"
      },
      "source": [
        "# max sequence length in our dataset\n",
        "\n",
        "tokens_len = []\n",
        "\n",
        "for text in reviews_df['content']:\n",
        "    tokens = tokenizer.encode(text, max_length=512)\n",
        "    tokens_len.append(len(tokens))\n",
        "\n",
        "sns.distplot(tokens_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fbe2b998f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1AYotUKXtv1"
      },
      "source": [
        "**CREATING TORCH DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDXKy5xzUhvM"
      },
      "source": [
        "class GooglePlayReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, review, target, tokenizer, max_len):\n",
        "        self.review = review\n",
        "        self.target = target\n",
        "        self.tokenzier = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.review)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.review[item])\n",
        "\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            review,\n",
        "            max_length=self.max_len,\n",
        "            add_special_tokens=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(self.target[item], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtjA50F1Xh78"
      },
      "source": [
        "# defining hyper-parameters\n",
        "\n",
        "MAX_LEN = 130\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ1QZLWxXh4U"
      },
      "source": [
        "df_train, df_test = train_test_split(reviews_df, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3UZI7ZnXpaQ"
      },
      "source": [
        "df_val = df_train.iloc[4000:, :]\n",
        "df_train = df_train.iloc[:4000, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLDsO0i2YKJP",
        "outputId": "1b537f7a-d729-4168-d3f2-bbc6b2d3d147"
      },
      "source": [
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4000, 11), (800, 11), (1200, 11))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32bD2woiaP06"
      },
      "source": [
        "**CREATING PYTORCH DATALOADER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XNEcvS_aFpv"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    gprd = GooglePlayReviewDataset(\n",
        "        review=reviews_df['content'].to_numpy(),\n",
        "        target=reviews_df['sentiment'].to_numpy(),\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=MAX_LEN\n",
        "    )\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset=gprd,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=4\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POXp2IjnaXy7"
      },
      "source": [
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpdok9CGXTDG"
      },
      "source": [
        "Taking a look at our TRAIN_DATA_LOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlVgSTkvaXuh",
        "outputId": "4ce5c9ac-fa21-4190-82dc-ca3b8e757a3d"
      },
      "source": [
        "data = next(iter(train_data_loader)) # returns the next batch of data i.e. 8 encoded reviews\n",
        "data.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-0ugNmJaXdC",
        "outputId": "f64879db-3c44-4722-fc8d-a51b18785f0c"
      },
      "source": [
        "print(data['input_ids'].shape) # shape = batch_size, feature, max_len\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 130])\n",
            "torch.Size([8, 130])\n",
            "torch.Size([8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85W347Y34fpb"
      },
      "source": [
        "Transformer Bert Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va6GlujEyNBj"
      },
      "source": [
        "bert_model = transformers.BertModel.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa7hRPCuaXYm"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask'],\n",
        "  return_dict=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQKElk5q8OxN"
      },
      "source": [
        "The last_hidden_state is a sequence of hidden states of the last layer of the model. Obtaining the pooled_output is done by applying the BertPooler on last_hidden_state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pV6_j9qcYZL",
        "outputId": "90b53bdd-7822-4144-a850-b0b2e9b87e29"
      },
      "source": [
        "last_hidden_state.shape, pooled_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 20, 768]), torch.Size([1, 768]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_mU0m7687t0"
      },
      "source": [
        "We have the hidden state for each of our 20 tokens (the length of our example sequence). 768 is the number of hidden units in the feedforward-networks : bert_model.config.hidden_size = 768.\n",
        "We need the pooled_output not last_hidden_state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfAw41a39cd0"
      },
      "source": [
        "BUILDING THE SENTIMENT CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cMNg_v3zt77"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(input_ids, attention_mask, return_dict=False)\n",
        "        output = self.dropout(pooled_output)\n",
        "        output = self.linear(output)\n",
        "        result = self.softmax(output)\n",
        "        return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jJ5cSX6AHPM",
        "outputId": "4db6133f-0da4-4f23-af88-0e588b88f20a"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Zxxanm9D2X"
      },
      "source": [
        "model = SentimentClassifier(len(output_classes))\n",
        "model = model.to(device) # sending model to GPU :D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4VuD29kA_9s"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOVoopUO9Dx6"
      },
      "source": [
        "optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "\n",
        "total_steps = len(train_data_loader)*EPOCHS\n",
        "\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxjVJyTH9Dvz"
      },
      "source": [
        "def train(model, data_loader, optimizer, loss_fn, device, scheduler, n_examples):\n",
        "    \n",
        "    model = model.train()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for dictionary in data_loader:\n",
        "        input_ids = dictionary['input_ids'].to(device)\n",
        "        attention_mask = dictionary['attention_mask'].to(device)\n",
        "        targets = dictionary['targets'].to(device)\n",
        "\n",
        "        results = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        output_class = torch.argmax(results, dim=1)\n",
        "\n",
        "        loss = loss_function(results, targets)\n",
        "\n",
        "        correct_predictions += torch.sum(output_class == targets)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    return correct_predictions.double()/n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmCmcZjgBtw2"
      },
      "source": [
        "def eval(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            targets = d['targets'].to(device)\n",
        "\n",
        "            results = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            output_class = torch.argmax(results, dim=1)\n",
        "\n",
        "            loss = loss_function(results, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(output_class == targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return correct_predictions.double()/n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HkFNe0kj9Sht",
        "outputId": "8142d35f-f405-4bab-834f-2abafa4469a8"
      },
      "source": [
        "MODEL_PATH = 'model_base_cased.bin'\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\"*10)\n",
        "\n",
        "    train_acc, train_loss = train(\n",
        "        model, \n",
        "        train_data_loader, \n",
        "        optimizer, \n",
        "        loss_function, \n",
        "        device, \n",
        "        scheduler, \n",
        "        len(df_train)\n",
        "    )\n",
        "\n",
        "    print(f\"Train loss : {train_loss}, Train Accuracy : {train_acc}\")\n",
        "\n",
        "    val_acc, val_loss = eval(\n",
        "        model, \n",
        "        val_data_loader, \n",
        "        loss_function, \n",
        "        device, \n",
        "        len(df_val)\n",
        "    )\n",
        "\n",
        "    print(f\"Val loss : {val_loss}, Val Accuracy : {val_acc}\")\n",
        "    print()\n",
        "\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "\n",
        "    if val_acc > best_acc:\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_acc = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1381433832645416, Train Accuracy : 0.3665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1028523078759511, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 2/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.136641446908315, Train Accuracy : 0.36625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1027538673877717, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 3/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1347591642538706, Train Accuracy : 0.3735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1021164059638977, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 4/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1330866839090983, Train Accuracy : 0.3665\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.101926366488139, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 5/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1302879874706269, Train Accuracy : 0.37625000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1016644287109374, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 6/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1284431379636128, Train Accuracy : 0.39\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1012469471295674, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 7/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1273677631219228, Train Accuracy : 0.399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1008720402717591, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 8/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1301302931308745, Train Accuracy : 0.37625000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1006396547953288, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 9/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1288182303905487, Train Accuracy : 0.381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1006144285202026, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 10/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1269140817324321, Train Accuracy : 0.38725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.100361796538035, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 11/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.124854060570399, Train Accuracy : 0.38975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.1000916134516399, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 12/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1265185118516285, Train Accuracy : 0.38725\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val loss : 1.0999382840792338, Val Accuracy : 2.5\n",
            "\n",
            "Epoch 13/30\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss : 1.1240217927296956, Train Accuracy : 0.40700000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-a56f53382034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-78c6da3e51b5>\u001b[0m in \u001b[0;36meval\u001b[0;34m(model, data_loader, loss_fn, device, n_examples)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1JG8ywkgRl_"
      },
      "source": [
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_isp6KpsbfLO"
      },
      "source": [
        "LOADING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwC9FF7l9XG6"
      },
      "source": [
        "model = SentimentClassifier(len(output_classes))\n",
        "model.load_state_dict(torch.load('model_base_cased.bin'))\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agnF4Imcayj"
      },
      "source": [
        "EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63mp3m80BZ93"
      },
      "source": [
        "def get_reviews(model, data_loader):\n",
        "    model = model.eval()\n",
        "\n",
        "    review_texts = []\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "    actual_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            texts = d['review_text']\n",
        "            input_ids = d['input_ids'].to(device)\n",
        "            attention_mask = d['attention_mask'].to(device)\n",
        "            targets = d['targets'].to(device)\n",
        "\n",
        "            results = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            output_class = torch.argmax(results, dim=1)\n",
        "\n",
        "            review_texts.extend(texts)\n",
        "            predictions.extend(output_class)\n",
        "            prediction_probs.extend(results)\n",
        "            actual_values.extend(targets)\n",
        "\n",
        "    predictions = torch.stack(predictions).cpu()\n",
        "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "    actual_values = torch.stack(actual_values).cpu()\n",
        "\n",
        "    return review_texts, predictions, prediction_probs, actual_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjf7fFa4eboy"
      },
      "source": [
        "test_acc, test_loss = eval(model, test_data_loader, loss_function, device, len(df_test))\n",
        "\n",
        "print(f\"Test Accuracy : {test_acc.item()}, Test Loss : {test_loss.item()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxcKvUNFeIg1"
      },
      "source": [
        "y_review_texts, y_pred, y_preds_probs, y_test = get_reviews(model, test_data_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNlEYphxeIcL"
      },
      "source": [
        "print(classification_report(y_test, y_pred, target_names=output_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H7jDA0df9P9"
      },
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uuYKgmhgtO"
      },
      "source": [
        "PREDICTION ON RAW TEXT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n_wRwKUhmni"
      },
      "source": [
        "raw_text = 'I played this game for 2 weeks and I found it quite good, the interface and the recoil seems playable.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uFAWcprhnSi"
      },
      "source": [
        "encoding_on_raw_text = tokenizer.encode_plus(\n",
        "    raw_text,\n",
        "    max_len=MAX_LEN,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=False,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXrIIWikhnOH"
      },
      "source": [
        "raw_input_ids = encoding_on_raw_text['input_ids'].to(device)\n",
        "raw_attention_mask = encoding_on_raw_text['attention_mask'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omGaDoOxims_"
      },
      "source": [
        "output = model(raw_input_ids, raw_attention_mask)\n",
        "prediction = torch.argmax(output, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG3fyasWi2XU"
      },
      "source": [
        "print(f\"Raw Text : {raw_text}\")\n",
        "print()\n",
        "print(f\"Sentiment : {output_classes[prediction]}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}