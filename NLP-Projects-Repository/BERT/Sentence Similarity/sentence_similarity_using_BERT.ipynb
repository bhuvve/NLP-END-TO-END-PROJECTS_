{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence similarity using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txEESvQvS5s"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfLs_UJQvTE1"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7IStj6uvTJN"
      },
      "source": [
        "sentences = [\n",
        "             \"Three years later, the coffin was still full of Jello.\",\n",
        "             \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
        "             \"The person box was packed with jelly many dozens of months later.\",\n",
        "             \"Standing on one's head at job interviews forms a lasting impression.\",\n",
        "             \"It took him a month to finish the meal.\",\n",
        "             \"Finishing the meal took him 3 weeks.\"\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6_zQDuzvTRp"
      },
      "source": [
        "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4QMUKcWvTT7"
      },
      "source": [
        "def compute_tokens(sentences, tokenizer):\n",
        "    \n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    \n",
        "    # encoding all sentences for bert input\n",
        "    for sentence in sentences:\n",
        "        sentence_encoding = tokenizer.encode_plus(\n",
        "            sentence,\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        input_ids.append(sentence_encoding['input_ids'][0])\n",
        "        attention_mask.append(sentence_encoding['attention_mask'][0])\n",
        "    \n",
        "    # stacking all the input_ids and attention_mask along 1 dim\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    attention_mask = torch.stack(attention_mask)\n",
        "    # final shape of input_ids & attention_mask = torch.Size([6, 128]), initially they were list.\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8j79hrAvTWO"
      },
      "source": [
        "def compute_sentence_vector(tokens, model):\n",
        "    \n",
        "    last_hidden_state, pooled_output = model(**tokens, return_dict=False)\n",
        "    '''\n",
        "    Now let's apply mean pooling on last_hidden_state vector of shape torch.Size([6,128,768])\n",
        "    to convert it into meaningful sentence embedding.\n",
        "\n",
        "    For this we need to create a sentence vector by multiplying the attention_mask\n",
        "    with last_hidden_state so that we ignore non-real tokens i.e. ignore padding tokens. \n",
        "    \n",
        "    The final sentence vector will have 768 embeddings for those\n",
        "    words where there was 1 else 0 = padding tokens.\n",
        "    In order to multiply, we need to expand attention_mask dim by 1 so that both becomes [6,128,768].\n",
        "    '''    \n",
        "    attention_mask = tokens['attention_mask'].unsqueeze(-1).expand(last_hidden_state.shape).float()\n",
        "    masked_embeddings = last_hidden_state * attention_mask \n",
        "\n",
        "    # applying mean pooling\n",
        "    '''\n",
        "    This pooling operation will take the mean of all token embeddings and compress them into a \n",
        "    single 768 vector space — creating a ‘sentence vector’.\n",
        "\n",
        "    At the same time, we can’t just take the mean activation as is. We need to consider \n",
        "    null padding tokens (which we should not include).\n",
        "    '''\n",
        "    summed = torch.sum(masked_embeddings, dim=1) # shape = [6,768]\n",
        "    counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9) # shape = [6,768]\n",
        "    mean_pooled_embedding = summed / counts # shape = [6, 768] i.e. our final sentence vector.\n",
        "\n",
        "    return mean_pooled_embedding"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBswqqpWvTYm"
      },
      "source": [
        "def compute_similarity(sentences, tokenizer, model):\n",
        "    \n",
        "    sentences_tokens = compute_tokens(sentences, tokenizer)\n",
        "    sentences_embeddings = compute_sentence_vector(sentences_tokens, model)\n",
        "    sentences_embeddings_detached = sentences_embeddings.detach().numpy()\n",
        "    similarity_scores = cosine_similarity([sentences_embeddings_detached[0]], sentences_embeddings_detached[1:])\n",
        "\n",
        "    d = {\n",
        "        'column-1': [sentences[0] for _ in range(len(sentences)-1)],\n",
        "        'column-2': [sent for sent in sentences[1:]],\n",
        "        'scores': similarity_scores[0]\n",
        "    }\n",
        "\n",
        "    output = pd.DataFrame(data=d)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsOqTYM_vTbq"
      },
      "source": [
        "output = compute_similarity(sentences, tokenizer, model)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4UbkWdKUvTgQ",
        "outputId": "bca3fbdd-9a7e-42ed-8420-b9d7964a66e2"
      },
      "source": [
        "output"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>column-1</th>\n",
              "      <th>column-2</th>\n",
              "      <th>scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Three years later, the coffin was still full o...</td>\n",
              "      <td>The fish dreamed of escaping the fishbowl and ...</td>\n",
              "      <td>0.330889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Three years later, the coffin was still full o...</td>\n",
              "      <td>The person box was packed with jelly many doze...</td>\n",
              "      <td>0.721926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Three years later, the coffin was still full o...</td>\n",
              "      <td>Standing on one's head at job interviews forms...</td>\n",
              "      <td>0.174755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Three years later, the coffin was still full o...</td>\n",
              "      <td>It took him a month to finish the meal.</td>\n",
              "      <td>0.447096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Three years later, the coffin was still full o...</td>\n",
              "      <td>Finishing the meal took him 3 weeks.</td>\n",
              "      <td>0.579585</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            column-1  ...    scores\n",
              "0  Three years later, the coffin was still full o...  ...  0.330889\n",
              "1  Three years later, the coffin was still full o...  ...  0.721926\n",
              "2  Three years later, the coffin was still full o...  ...  0.174755\n",
              "3  Three years later, the coffin was still full o...  ...  0.447096\n",
              "4  Three years later, the coffin was still full o...  ...  0.579585\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}