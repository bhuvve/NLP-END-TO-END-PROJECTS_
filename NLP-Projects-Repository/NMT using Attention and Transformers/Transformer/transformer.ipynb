{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRKUDX4f6Zwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cd gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iM0wJzq9oOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x34jYyTAra_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/gdrive/My Drive/Hindi_English_Truncated_Corpus.csv'\n",
        "df = pd.read_csv(data_path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5huYZR4Av0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nan_values = df[df.isnull().any(axis=1)].index\n",
        "df = df.drop(nan_values)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlYyYqiSENj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punc = set(string.punctuation)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwrEHAeNFQF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#lower casing\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x:x.lower())\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x:x.lower())\n",
        "#removing digits\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\"[0-9]\", \"\", x))\n",
        "#removing space\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x: x.strip())\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: x.strip())\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "#removing punctuation\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: ''.join(char for char in x if char not in punc))\n",
        "#adding start and end sequence\n",
        "df['english_sentence'] = df['english_sentence'].apply(lambda x: 'start_ ' + x + ' _end')\n",
        "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: 'start_ ' + x + ' _end')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6WseXgvFbsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_data = df['english_sentence'].to_list()\n",
        "hindi_data = df['hindi_sentence'].to_list()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr_t9kXBFhQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#max length of sentences in each data\n",
        "eng, hin = [], []\n",
        "for l in english_data:\n",
        "    eng.append(len(l.split(' ')))\n",
        "\n",
        "for l in hindi_data:\n",
        "    hin.append(len(l.split(' ')))\n",
        "\n",
        "max_eng_sentence_length = 50 #max(eng)\n",
        "max_hindi_sentence_length = 60 #max(hin)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3xp_ewFkG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_tokenizer = Tokenizer(filters='')\n",
        "hindi_tokenizer = Tokenizer(filters='')\n",
        "\n",
        "english_tokenizer.fit_on_texts(english_data)\n",
        "hindi_tokenizer.fit_on_texts(hindi_data)\n",
        "\n",
        "#getting vocab sizes\n",
        "english_vocab = len(english_tokenizer.word_index)+1\n",
        "hindi_vocab = len(hindi_tokenizer.word_index)+1\n",
        "\n",
        "#converting to sequences\n",
        "english_encoded_text = english_tokenizer.texts_to_sequences(english_data)\n",
        "hindi_encoded_text = hindi_tokenizer.texts_to_sequences(hindi_data)\n",
        "\n",
        "#padding\n",
        "english_padded_text = pad_sequences(english_encoded_text, maxlen=max_eng_sentence_length, padding='post')\n",
        "hindi_padded_text = pad_sequences(hindi_encoded_text, maxlen=max_hindi_sentence_length, padding='post')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(english_padded_text, hindi_padded_text, test_size=0.2)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIN4xSGVFxd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#hyperparameters\n",
        "units = 1024\n",
        "dims = 512\n",
        "max_positional_encoding = 10000"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhEA1dn-GLf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#positional encoding\n",
        "def get_angles(position, i, dims):\n",
        "    angle_rates = 1/np.power(10000, (2*(i//2))/np.float32(dims))\n",
        "    return position*angle_rates\n",
        "\n",
        "def positional_encoding(position, dims):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(dims)[np.newaxis, :], dims)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) #sin to even posi\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) #cos to odd posi\n",
        "    position_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(position_encoding, dtype=tf.float32)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GQiYwJfHwLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transformer architecture\n",
        "def Encoder(vocab_size, dims, input_):\n",
        "\n",
        "    embedding_layer = layers.Embedding(vocab_size, dims)(input_)\n",
        "    #positional encoding\n",
        "    scaling_factor = K.constant(np.sqrt(dims), shape=(1,1,1))\n",
        "    x = layers.Multiply()([embedding_layer, scaling_factor])\n",
        "    pos = positional_encoding(max_positional_encoding, dims)\n",
        "    x = layers.Add()([x, pos[: , :tf.shape(x)[1], :]])\n",
        "    #self-attention\n",
        "    query_vector = layers.Dense(dims)(x)\n",
        "    key_vector = layers.Dense(dims)(x)\n",
        "    value_vector = layers.Dense(dims)(x)\n",
        "    attention = layers.Attention()([query_vector, value_vector, key_vector])\n",
        "    attention = layers.Dense(dims)(attention)\n",
        "    x = layers.Add()([x, attention]) #residual connection\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    #feed forward neural network\n",
        "    fully_connected = layers.Dense(units=units, activation='relu')(x)\n",
        "    fully_connected = layers.Dense(units=dims)(fully_connected)\n",
        "    x = layers.Add()([x, fully_connected]) #residual connection\n",
        "    encoder = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    return encoder\n",
        "\n",
        "def Decoder(vocab_size, dims, encoder, target):\n",
        "    \n",
        "    embedding_layer = layers.Embedding(vocab_size, dims)(target)\n",
        "    #positional encoding\n",
        "    scaling_factor = K.constant(np.sqrt(dims), shape=(1,1,1))\n",
        "    x = layers.Multiply()([embedding_layer, scaling_factor])\n",
        "    pos = positional_encoding(max_positional_encoding, dims)\n",
        "    x = layers.Add()([x, pos[: , :tf.shape(x)[1], :]])\n",
        "    #self-attention\n",
        "    query_vector = layers.Dense(dims)(x)\n",
        "    key_vector = layers.Dense(dims)(x)\n",
        "    value_vector = layers.Dense(dims)(x)\n",
        "    attention = layers.Attention(causal=True)([query_vector, value_vector, key_vector])\n",
        "    attention = layers.Dense(dims)(attention)\n",
        "    x = layers.Add()([x, attention]) #residual connection\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    #encoder-decoder attention\n",
        "    query = layers.Dense(dims)(x)\n",
        "    key = layers.Dense(dims)(encoder)\n",
        "    value = layers.Dense(dims)(encoder)\n",
        "    attention = layers.Attention()([query, value, key])\n",
        "    attention = layers.Dense(dims)(attention)\n",
        "    x = layers.Add()([x, attention]) #residual connection\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    #feed forward neural network\n",
        "    fully_connected = layers.Dense(units=units, activation='relu')(x)\n",
        "    fully_connected = layers.Dense(dims)(fully_connected)\n",
        "    x = layers.Add()([x, fully_connected]) #residual connection\n",
        "    decoder = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    decoder = layers.Dense(units=vocab_size)(decoder)\n",
        "\n",
        "    return decoder"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzYKF-8GHxdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ = layers.Input(shape=(None,))\n",
        "target = layers.Input(shape=(None,))\n",
        "encoder = Encoder(english_vocab, dims, input_)\n",
        "decoder = Decoder(hindi_vocab, dims, encoder, target)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJF9RlYRHxWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs=[input_, target], outputs=decoder)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn_IB0NPOsEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(model, to_file='/content/gdrive/My Drive/model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxYUVebNOtrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = Adam(0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "metrics = SparseCategoricalAccuracy()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF0Yb-lVPrBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7acDszMR4rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint('/content/gdrive/My Drive/checkpoints/transformer.h5',\n",
        "                             monitor='val_loss',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "callbacks = [checkpoint]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RazeL11FQLZi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "5abb8e0a-5193-4e77-acdb-dbad7709c963"
      },
      "source": [
        "hist = model.fit(x=[x_train, y_train[:, :-1]], y=y_train[:, 1:], batch_size=128, epochs=20, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "798/798 [==============================] - ETA: 0s - loss: 1.9977 - sparse_categorical_accuracy: 0.7459WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "798/798 [==============================] - 1236s 2s/step - loss: 1.9977 - sparse_categorical_accuracy: 0.7459\n",
            "Epoch 2/20\n",
            "798/798 [==============================] - ETA: 0s - loss: 1.5509 - sparse_categorical_accuracy: 0.7686WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "798/798 [==============================] - 1244s 2s/step - loss: 1.5509 - sparse_categorical_accuracy: 0.7686\n",
            "Epoch 3/20\n",
            "798/798 [==============================] - ETA: 0s - loss: 1.4226 - sparse_categorical_accuracy: 0.7761WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "798/798 [==============================] - 1249s 2s/step - loss: 1.4226 - sparse_categorical_accuracy: 0.7761\n",
            "Epoch 4/20\n",
            "798/798 [==============================] - ETA: 0s - loss: 1.3495 - sparse_categorical_accuracy: 0.7814WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "798/798 [==============================] - 1244s 2s/step - loss: 1.3495 - sparse_categorical_accuracy: 0.7814\n",
            "Epoch 5/20\n",
            "798/798 [==============================] - ETA: 0s - loss: 1.3164 - sparse_categorical_accuracy: 0.7838WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "798/798 [==============================] - 1237s 2s/step - loss: 1.3164 - sparse_categorical_accuracy: 0.7838\n",
            "Epoch 6/20\n",
            "771/798 [===========================>..] - ETA: 42s - loss: 1.2811 - sparse_categorical_accuracy: 0.7869"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-Wcniy5RM9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izF-PCJ8RVm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}