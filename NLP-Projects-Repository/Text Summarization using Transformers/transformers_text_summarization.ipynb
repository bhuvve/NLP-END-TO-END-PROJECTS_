{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformers text summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BNIrVaS1gh1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL2MSEL53TIm"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVeyO1je14Nw"
      },
      "source": [
        "data = pd.read_excel('/content/gdrive/My Drive/Inshorts Cleaned Data.xlsx')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw5DRGDa14KB",
        "outputId": "8e3d7650-cf1b-4c2b-b7f3-b62b07f8dc19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "      <th>Source</th>\n",
              "      <th>Time</th>\n",
              "      <th>Publish Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "      <td>The New Indian Express</td>\n",
              "      <td>09:25:00</td>\n",
              "      <td>2017-03-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "      <td>Outlook</td>\n",
              "      <td>22:18:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "      <td>Hindustan Times</td>\n",
              "      <td>23:39:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>23:08:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "      <td>YouTube</td>\n",
              "      <td>23:24:00</td>\n",
              "      <td>2017-03-25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ... Publish Date\n",
              "0  4 ex-bank officials booked for cheating bank o...  ...   2017-03-26\n",
              "1     Supreme Court to go paperless in 6 months: CJI  ...   2017-03-25\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  ...   2017-03-25\n",
              "3  Why has Reliance been barred from trading in f...  ...   2017-03-25\n",
              "4  Was stopped from entering my own studio at Tim...  ...   2017-03-25\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo914HNx14Hp",
        "outputId": "e4235b3b-c836-4dd5-8432-a3c27c183ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.drop(columns=['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline                                              Short\n",
              "0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n",
              "1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n",
              "3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
              "4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnH5s5EF14FX"
      },
      "source": [
        "new = {'Headline': 'Summary', 'Short': 'News'}\n",
        "data = data.rename(new, axis=1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apR2GL8B14C1",
        "outputId": "3f3470f2-c21b-4c2a-9e6f-61f666b11861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Summary</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             Summary                                               News\n",
              "0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n",
              "1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n",
              "3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
              "4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TapnLRcA14Ah"
      },
      "source": [
        "#preprocessing\n",
        "data['Summary'] = data['Summary'].apply(lambda x: '<start> '+x+' <end>')\n",
        "#removing extra spaces\n",
        "data['News'] = data['News'].apply(lambda x: x.strip())\n",
        "data['Summary'] = data['Summary'].apply(lambda x: x.strip())\n",
        "#keeping only alphabets and numbers\n",
        "data['News'] = data['News'].apply(lambda x: re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", \" \", x))\n",
        "data['Summary'] = data['Summary'].apply(lambda x: re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^<>A-Za-z0-9]+\", \" \", x))\n",
        "#lower casing\n",
        "data['News'] = data['News'].apply(lambda x: x.lower())\n",
        "data['Summary'] = data['Summary'].apply(lambda x: x.lower())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZOAlUbx6Y6Z"
      },
      "source": [
        "newsTokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>')\n",
        "summaryTokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\", oov_token='<unk>')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5S9Ea2e6Y3B"
      },
      "source": [
        "newsTokenizer.fit_on_texts(data['News'])\n",
        "summaryTokenizer.fit_on_texts(data['Summary'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omQWrVXw6Y1d"
      },
      "source": [
        "inputs = newsTokenizer.texts_to_sequences(data['News'])\n",
        "targets = summaryTokenizer.texts_to_sequences(data['Summary'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9buB21Em6YzA",
        "outputId": "b66472b3-8cb8-4efa-a0a8-bc61236d7e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print('News example: ', data['News'][0])\n",
        "print('News to sequences: ', inputs[0])\n",
        "print()\n",
        "print('Summary example: ', data['Summary'][0])\n",
        "print('Summary to sequences: ', targets[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News example:  the cbi on saturday booked four former officials of syndicate bank and six others for cheating forgery criminal conspiracy and causing 209 crore loss to the state run bank the accused had availed home loans and credit from syndicate bank on the basis of forged and fabricated documents these funds were fraudulently transferred to the companies owned by the accused persons \n",
            "News to sequences:  [2, 1146, 9, 120, 1917, 156, 112, 172, 6, 11820, 181, 8, 212, 331, 12, 3897, 17598, 1471, 3469, 8, 1909, 13127, 56, 730, 3, 2, 64, 295, 181, 2, 241, 35, 11821, 243, 1627, 8, 1807, 21, 11820, 181, 9, 2, 1440, 6, 7438, 8, 13128, 1515, 292, 868, 39, 17599, 3736, 3, 2, 453, 919, 17, 2, 241, 1718]\n",
            "\n",
            "Summary example:  <start> 4 ex bank officials booked for cheating bank of 209 crore <end>\n",
            "Summary to sequences:  [2, 59, 130, 133, 734, 713, 8, 2313, 133, 9, 13316, 46, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwps2DOPsvoa"
      },
      "source": [
        "news_vocab_size = len(newsTokenizer.word_index)+1\n",
        "summary_vocab_size = len(summaryTokenizer.word_index)+1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT60t3jUwPlT",
        "outputId": "1285a62b-1993-4ff9-fae7-6ed3d5ef5e63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('News vocabulary size: ', news_vocab_size)\n",
        "print('Summary vocabulary size:', summary_vocab_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News vocabulary size:  68204\n",
            "Summary vocabulary size: 28284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bijggi9swPh0"
      },
      "source": [
        "#getting appropriate lens \n",
        "news_lengths = pd.Series([len(x) for x in data['News']])\n",
        "summary_lengths = pd.Series([len(x) for x in data['Summary']])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VElJUPeuwPfO",
        "outputId": "02461496-8893-45f9-cb3e-031cbe2a481e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "news_lengths.describe()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean       358.166485\n",
              "std         24.743317\n",
              "min        213.000000\n",
              "25%        342.000000\n",
              "50%        359.000000\n",
              "75%        377.000000\n",
              "max        436.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vFA814uwPdk",
        "outputId": "425fbf28-a2ed-4a4b-e978-d24d09ae92bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "summary_lengths.describe()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    55104.000000\n",
              "mean        64.479040\n",
              "std          6.762691\n",
              "min         15.000000\n",
              "25%         60.000000\n",
              "50%         64.000000\n",
              "75%         70.000000\n",
              "max         87.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "palxqkbHwPac"
      },
      "source": [
        "#taking values > and round figured to 75th percentile\n",
        "news_max_len = 400\n",
        "summary_max_len = 75"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAmXtRWRxUuK"
      },
      "source": [
        "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=news_max_len, padding='post', truncating='post')\n",
        "padded_targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=summary_max_len, padding='post', truncating='post')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_IyJP95xUqU"
      },
      "source": [
        "#creating dataset pipeline\n",
        "final_news_data = tf.cast(padded_inputs, dtype=tf.int32)\n",
        "final_summary_data = tf.cast(padded_targets, dtype=tf.int32)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIW-kDT4xUnc"
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC88cBLCxUlO"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((final_news_data, final_summary_data)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3uEeF2F6Fqy"
      },
      "source": [
        "BUILDING TRANSFORMER MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUZa0D3yyq9s"
      },
      "source": [
        "#positional encoding\n",
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1/np.power(10000, (2*(i//2))/np.float32(d_model))\n",
        "    return position*angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "#creating padding for padded sequences\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "#creating look ahead mask for masking future words from contributing in prediction of current words in self attention\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "#scaled dot product\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights\n",
        "\n",
        "#Multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights\n",
        "\n",
        "#Feed-forward network\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)])\n",
        "\n",
        "#Fundamental unit of transformer encoder\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n",
        "\n",
        "#Fundamental unit of transformer decoder\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "#Encoder having multiple encoder layers\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x\n",
        "\n",
        "#Decoder having multiple decoder layers\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights\n",
        "\n",
        "#The Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-_8VHD0yrtx"
      },
      "source": [
        "#hyper-parameter\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 20"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfsM1-LXyrrH"
      },
      "source": [
        "#adam optimizer with custom learning rate scheduler\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqBeDc1ryrpK"
      },
      "source": [
        "#defining loss and loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujW60TcpyrnF"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    news_vocab_size, \n",
        "    summary_vocab_size, \n",
        "    pe_input=news_vocab_size, \n",
        "    pe_target=summary_vocab_size,)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brjSWMEu8m3t"
      },
      "source": [
        "#creating masks for training\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AldVKRej8m0T"
      },
      "source": [
        "#creating checkpoints for saving model config and weights\n",
        "checkpoint_path = \"/content/gdrive/My Drive/Transformer/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MWievwL8mu0"
      },
      "source": [
        "#training steps\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRxhMPJ9qIZ"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS4ZRPDK8mqX",
        "outputId": "ab5a4b51-5898-446e-bfba-f59fb512fee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 429 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for this epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.2383\n",
            "Epoch 1 Batch 429 Loss 9.1381\n",
            "Epoch 1 Batch 858 Loss 8.1922\n",
            "Epoch 1 Loss 8.1895\n",
            "Time taken for 1 epoch: 432.29950428009033 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 7.1133\n",
            "Epoch 2 Batch 429 Loss 6.7633\n",
            "Epoch 2 Batch 858 Loss 6.5603\n",
            "Epoch 2 Loss 6.5597\n",
            "Time taken for 1 epoch: 419.80424332618713 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 6.1679\n",
            "Epoch 3 Batch 429 Loss 6.0350\n",
            "Epoch 3 Batch 858 Loss 5.8951\n",
            "Epoch 3 Loss 5.8944\n",
            "Time taken for 1 epoch: 418.21786737442017 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.6558\n",
            "Epoch 4 Batch 429 Loss 5.5243\n",
            "Epoch 4 Batch 858 Loss 5.4266\n",
            "Epoch 4 Loss 5.4262\n",
            "Time taken for 1 epoch: 417.3145360946655 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.2704\n",
            "Epoch 5 Batch 429 Loss 5.1326\n",
            "Epoch 5 Batch 858 Loss 5.0677\n",
            "Saving checkpoint for epoch 5 at /content/gdrive/My Drive/Transformer/ckpt-1\n",
            "Epoch 5 Loss 5.0674\n",
            "Time taken for 1 epoch: 419.02750301361084 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 5.0244\n",
            "Epoch 6 Batch 429 Loss 4.7660\n",
            "Epoch 6 Batch 858 Loss 4.6989\n",
            "Epoch 6 Loss 4.6987\n",
            "Time taken for 1 epoch: 420.0102150440216 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.5594\n",
            "Epoch 7 Batch 429 Loss 4.4127\n",
            "Epoch 7 Batch 858 Loss 4.3672\n",
            "Epoch 7 Loss 4.3670\n",
            "Time taken for 1 epoch: 418.75036120414734 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 4.3401\n",
            "Epoch 8 Batch 429 Loss 4.1234\n",
            "Epoch 8 Batch 858 Loss 4.0902\n",
            "Epoch 8 Loss 4.0903\n",
            "Time taken for 1 epoch: 420.1379888057709 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 3.9613\n",
            "Epoch 9 Batch 429 Loss 3.8873\n",
            "Epoch 9 Batch 858 Loss 3.8706\n",
            "Epoch 9 Loss 3.8710\n",
            "Time taken for 1 epoch: 418.34821939468384 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.8075\n",
            "Epoch 10 Batch 429 Loss 3.6902\n",
            "Epoch 10 Batch 858 Loss 3.6963\n",
            "Saving checkpoint for epoch 10 at /content/gdrive/My Drive/Transformer/ckpt-2\n",
            "Epoch 10 Loss 3.6961\n",
            "Time taken for 1 epoch: 419.0134255886078 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.5962\n",
            "Epoch 11 Batch 429 Loss 3.5433\n",
            "Epoch 11 Batch 858 Loss 3.5473\n",
            "Epoch 11 Loss 3.5475\n",
            "Time taken for 1 epoch: 417.89524126052856 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.3677\n",
            "Epoch 12 Batch 429 Loss 3.3991\n",
            "Epoch 12 Batch 858 Loss 3.4121\n",
            "Epoch 12 Loss 3.4122\n",
            "Time taken for 1 epoch: 417.8354516029358 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.3174\n",
            "Epoch 13 Batch 429 Loss 3.2711\n",
            "Epoch 13 Batch 858 Loss 3.2817\n",
            "Epoch 13 Loss 3.2819\n",
            "Time taken for 1 epoch: 417.92789340019226 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.2159\n",
            "Epoch 14 Batch 429 Loss 3.1457\n",
            "Epoch 14 Batch 858 Loss 3.1637\n",
            "Epoch 14 Loss 3.1639\n",
            "Time taken for 1 epoch: 418.2911961078644 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 3.1223\n",
            "Epoch 15 Batch 429 Loss 3.0360\n",
            "Epoch 15 Batch 858 Loss 3.0562\n",
            "Saving checkpoint for epoch 15 at /content/gdrive/My Drive/Transformer/ckpt-3\n",
            "Epoch 15 Loss 3.0563\n",
            "Time taken for 1 epoch: 419.43438386917114 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 3.0693\n",
            "Epoch 16 Batch 429 Loss 2.9327\n",
            "Epoch 16 Batch 858 Loss 2.9582\n",
            "Epoch 16 Loss 2.9586\n",
            "Time taken for 1 epoch: 417.723858833313 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.9366\n",
            "Epoch 17 Batch 429 Loss 2.8425\n",
            "Epoch 17 Batch 858 Loss 2.8646\n",
            "Epoch 17 Loss 2.8650\n",
            "Time taken for 1 epoch: 417.27602100372314 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 2.7509\n",
            "Epoch 18 Batch 429 Loss 2.7497\n",
            "Epoch 18 Batch 858 Loss 2.7880\n",
            "Epoch 18 Loss 2.7881\n",
            "Time taken for 1 epoch: 417.45951890945435 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.6456\n",
            "Epoch 19 Batch 429 Loss 2.6755\n",
            "Epoch 19 Batch 858 Loss 2.7113\n",
            "Epoch 19 Loss 2.7118\n",
            "Time taken for 1 epoch: 418.8503952026367 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.5640\n",
            "Epoch 20 Batch 429 Loss 2.6050\n",
            "Epoch 20 Batch 858 Loss 2.6425\n",
            "Saving checkpoint for epoch 20 at /content/gdrive/My Drive/Transformer/ckpt-4\n",
            "Epoch 20 Loss 2.6428\n",
            "Time taken for 1 epoch: 421.2965476512909 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wn0dali-5sc"
      },
      "source": [
        "INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVjd-0l89rhb"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = newsTokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, \n",
        "                                                                   maxlen=news_max_len, \n",
        "                                                                   padding='post', \n",
        "                                                                   truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summaryTokenizer.word_index[\"<start>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(summary_max_len):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summaryTokenizer.word_index[\"<end>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def summarize(input_document):\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <start> token\n",
        "    return summaryTokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeiXjzGA9rcO",
        "outputId": "2efbc257-3ac5-4718-a14f-1f287145e53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "summary = summarize(\"It's clear that sexual education is completely vital to the public-school curriculum. Not only does this lead to a better understanding of human development and human sexuality, but awareness and sex education also reduce the rates of teen pregnancy. Studies have shown that comprehensive sexual education increases the age of when teens have sex for the first time. Learning about contraception and how to use contraception correctly ultimately leads to lower rates of STDs. Lastly, comprehensive sex education also teaches students about consensual sex, and will hopefully lead to healthier sexual relationships and lower rates of sexual assault in the future. Not only should sex education be taught in public schools, but it should be mandatory for all public-school systems.\")\n",
        "print(summary)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "women will shut down jobs as they are work maneka\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eumrb0I7esy9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
